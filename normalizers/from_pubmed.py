#!/usr/bin/env python3
"""
Normalizer: PubMed -> model/pubmed.cue

Queries NCBI PubMed via E-utilities (esearch + esummary) for each gene,
counting craniofacial/neural-crest publications and fetching top 3 recent.

Cached at data/pubmed/pubmed_cache.json. Respects NCBI rate limits.

Usage:
    python3 normalizers/from_pubmed.py
"""

import json
import sys
import time
import urllib.parse
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(REPO_ROOT / "normalizers"))

from genes import GENES
from utils import fetch_json_with_retry

CACHE_DIR = REPO_ROOT / "data" / "pubmed"
CACHE_FILE = CACHE_DIR / "pubmed_cache.json"
OUTPUT_FILE = REPO_ROOT / "model" / "pubmed.cue"

# Search for gene + craniofacial context
SEARCH_TERM = '({gene}[Title/Abstract]) AND (craniofacial OR "neural crest" OR dental OR orofacial OR craniosynostosis)'

ESEARCH_URL = (
    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    "?db=pubmed&retmode=json&retmax=0"
    "&term={term}"
)

# For recent count: add date filter
ESEARCH_RECENT_URL = (
    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    "?db=pubmed&retmode=json&retmax=0"
    "&term={term}&mindate=2021&maxdate=2026&datetype=pdat"
)

# Get top 3 recent PMIDs
ESEARCH_TOP_URL = (
    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    "?db=pubmed&retmode=json&retmax=3&sort=date"
    "&term={term}"
)

ESUMMARY_URL = (
    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"
    "?db=pubmed&retmode=json&id={ids}"
)

REQUEST_DELAY = 0.35


def fetch_json(url: str) -> dict | None:
    try:
        return fetch_json_with_retry(url, headers={"Accept": "application/json"})
    except Exception as e:
        print(f"  WARNING: request failed: {e}", file=sys.stderr)
        return None


def query_pubmed_gene(symbol: str) -> dict | None:
    term = SEARCH_TERM.format(gene=symbol)
    encoded_term = urllib.parse.quote(term, safe="")

    # Total count
    url = ESEARCH_URL.format(term=encoded_term)
    data = fetch_json(url)
    if data is None or "esearchresult" not in data:
        return None
    total = int(data["esearchresult"].get("count", 0))
    time.sleep(REQUEST_DELAY)

    # Recent count (last 5 years)
    url = ESEARCH_RECENT_URL.format(term=encoded_term)
    data = fetch_json(url)
    recent = 0
    if data and "esearchresult" in data:
        recent = int(data["esearchresult"].get("count", 0))
    time.sleep(REQUEST_DELAY)

    # Top 3 recent papers
    url = ESEARCH_TOP_URL.format(term=encoded_term)
    data = fetch_json(url)
    papers = []
    if data and "esearchresult" in data:
        id_list = data["esearchresult"].get("idlist", [])
        if id_list:
            time.sleep(REQUEST_DELAY)
            ids_param = ",".join(id_list[:3])
            url = ESUMMARY_URL.format(ids=ids_param)
            sdata = fetch_json(url)
            if sdata and "result" in sdata:
                for uid in sdata["result"].get("uids", [])[:3]:
                    entry = sdata["result"].get(uid, {})
                    title = entry.get("title", "")
                    pubdate = entry.get("pubdate", "")
                    year = 0
                    if pubdate:
                        try:
                            year = int(pubdate[:4])
                        except ValueError:
                            pass
                    if title:
                        papers.append({
                            "title": title,
                            "pmid": uid,
                            "year": year,
                        })

    return {"pubmed_total": total, "pubmed_recent": recent, "papers": papers}


def load_cache() -> dict:
    if CACHE_FILE.exists():
        with open(CACHE_FILE) as f:
            return json.load(f)
    return {}


def save_cache(cache: dict) -> None:
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    with open(CACHE_FILE, "w") as f:
        json.dump(cache, f, indent=2, sort_keys=True)


def escape_cue_string(s: str) -> str:
    return s.replace("\\", "\\\\").replace('"', '\\"')


def generate_cue(pubmed_data: dict) -> str:
    lines = [
        "package froq",
        "",
        "// PubMed: craniofacial publication data for neural crest genes.",
        "// Source: NCBI PubMed via E-utilities (esearch + esummary)",
        f"// Generated by normalizers/from_pubmed.py -- {len(pubmed_data)} genes",
        "",
        "genes: {",
    ]

    for symbol in sorted(pubmed_data.keys()):
        entry = pubmed_data[symbol]
        ncbi_id = GENES[symbol]["ncbi"]

        lines.append(f'\t"{symbol}": {{')
        lines.append(f"\t\t_in_pubmed:     true")
        lines.append(f'\t\tpubmed_gene_id: "{ncbi_id}"')
        lines.append(f"\t\tpubmed_total:   {entry['pubmed_total']}")
        lines.append(f"\t\tpubmed_recent:  {entry['pubmed_recent']}")

        papers = entry.get("papers", [])
        if papers:
            lines.append(f"\t\tpubmed_papers: [")
            for p in papers:
                title = escape_cue_string(p["title"])
                lines.append(f"\t\t\t{{")
                lines.append(f'\t\t\t\ttitle: "{title}"')
                lines.append(f'\t\t\t\tpmid:  "{p["pmid"]}"')
                lines.append(f"\t\t\t\tyear:  {p['year']}")
                lines.append(f"\t\t\t}},")
            lines.append(f"\t\t]")

        lines.append(f"\t}}")

    lines.append("}")
    lines.append("")
    return "\n".join(lines)


def main():
    print(f"from_pubmed: querying PubMed for {len(GENES)} genes...")
    cache = load_cache()
    pubmed_data = {}
    fetched = 0

    for symbol in sorted(GENES.keys()):
        if symbol in cache:
            print(f"  {symbol}: cached ({cache[symbol]['pubmed_total']} pubs)")
            pubmed_data[symbol] = cache[symbol]
            continue

        print(f"  {symbol}: querying PubMed...", end=" ", flush=True)
        result = query_pubmed_gene(symbol)
        time.sleep(REQUEST_DELAY)

        if result is None:
            print("FAILED")
            continue

        print(f"{result['pubmed_total']} total, {result['pubmed_recent']} recent")
        pubmed_data[symbol] = result
        cache[symbol] = result
        fetched += 1

    save_cache(cache)

    cue_source = generate_cue(pubmed_data)
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, "w") as f:
        f.write(cue_source)

    print(f"from_pubmed: wrote {OUTPUT_FILE} ({len(pubmed_data)} genes)")
    print(f"  {fetched} fetched, {len(pubmed_data) - fetched} cached")


if __name__ == "__main__":
    main()
