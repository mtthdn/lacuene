#!/usr/bin/env python3
"""
Normalizer: Orphanet -> model/orphanet.cue

Downloads the Orphadata product6 bulk file (gene-disease associations),
parses it, and filters to our 95 neural crest genes. Each gene gets a list
of associated rare disorders with OrphaCode and disorder name.

The product6 XML maps genes to disorders. We download it once and cache
locally since this data changes infrequently (~quarterly).

Primary source: http://www.orphadata.org/data/xml/en_product6.xml
Fallback: per-gene queries via Orphanet API (requires no auth for search)

Usage:
    python3 normalizers/from_orphanet.py
"""

import json
import sys
import xml.etree.ElementTree as ET
from pathlib import Path

# Resolve paths relative to repo root (parent of normalizers/)
REPO_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(REPO_ROOT / "normalizers"))

from genes import GENES
from pipeline import PipelineReport, escape_cue_string
from utils import fetch_with_retry

CACHE_DIR = REPO_ROOT / "data" / "orphanet"
CACHE_FILE = CACHE_DIR / "orphanet_cache.json"
XML_CACHE = CACHE_DIR / "en_product6.xml"
OUTPUT_FILE = REPO_ROOT / "model" / "orphanet.cue"

ORPHADATA_URL = "http://www.orphadata.org/data/xml/en_product6.xml"


def download_product6() -> str:
    """
    Download the Orphadata product6 XML (gene-disease associations).
    Caches the raw XML locally. Returns the XML text.
    """
    if XML_CACHE.exists():
        print(f"  using cached XML: {XML_CACHE}")
        return XML_CACHE.read_text(encoding="utf-8")

    print(f"  downloading {ORPHADATA_URL} ...")
    resp = fetch_with_retry(ORPHADATA_URL, max_retries=3, backoff_base=3.0)
    xml_text = resp.text

    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    XML_CACHE.write_text(xml_text, encoding="utf-8")
    print(f"  cached XML: {XML_CACHE} ({len(xml_text)} bytes)")
    return xml_text


def parse_product6(xml_text: str) -> dict:
    """
    Parse product6 XML and extract gene-disorder associations.

    Returns dict keyed by gene symbol, each value is a list of dicts:
        { "orpha_code": str, "name": str }
    """
    gene_disorders = {}

    root = ET.fromstring(xml_text)
    for disorder in root.iter("Disorder"):
        orpha_code_el = disorder.find("OrphaCode")
        name_el = disorder.find("Name")
        if orpha_code_el is None or name_el is None:
            continue

        orpha_code = orpha_code_el.text or ""
        disorder_name = name_el.text or ""

        for assoc in disorder.iter("DisorderGeneAssociation"):
            gene_el = assoc.find("Gene")
            if gene_el is None:
                continue
            symbol_el = gene_el.find("Symbol")
            if symbol_el is None or not symbol_el.text:
                continue

            symbol = symbol_el.text.strip()
            if symbol not in gene_disorders:
                gene_disorders[symbol] = []

            # Avoid duplicate entries for the same disorder
            existing_codes = {d["orpha_code"] for d in gene_disorders[symbol]}
            if orpha_code not in existing_codes:
                gene_disorders[symbol].append({
                    "orpha_code": orpha_code,
                    "name": disorder_name,
                })

    return gene_disorders


def load_cache() -> dict:
    """Load cached Orphanet data if available."""
    if CACHE_FILE.exists():
        with open(CACHE_FILE) as f:
            return json.load(f)
    return {}


def save_cache(cache: dict) -> None:
    """Persist the Orphanet cache to disk."""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    with open(CACHE_FILE, "w") as f:
        json.dump(cache, f, indent=2, sort_keys=True)
    print(f"  cached: {CACHE_FILE}")


def generate_cue(orphanet_data: dict) -> str:
    """Generate CUE source from Orphanet data, keyed by HGNC symbol."""
    gene_count = len(orphanet_data)
    lines = [
        "package lacuene",
        "",
        "// Orphanet: rare disease gene-disorder associations for neural crest genes.",
        "// Source: Orphadata product6 (en_product6.xml)",
        f"// Generated by normalizers/from_orphanet.py -- {gene_count} genes",
        "",
        "genes: {",
    ]

    for symbol in sorted(orphanet_data.keys()):
        entry = orphanet_data[symbol]
        disorders = entry.get("disorders", [])
        orpha_id = entry.get("orphanet_id", "")

        lines.append(f'\t"{symbol}": {{')
        lines.append(f"\t\t_in_orphanet: true")
        lines.append(f'\t\torphanet_id:  "{escape_cue_string(orpha_id)}"')

        if disorders:
            lines.append(f"\t\torphanet_disorders: [")
            for d in disorders:
                code = escape_cue_string(d["orpha_code"])
                name = escape_cue_string(d["name"])
                lines.append(f'\t\t\t{{orpha_code: "{code}", name: "{name}"}},')
            lines.append(f"\t\t]")

        lines.append(f"\t}}")

    lines.append("}")
    lines.append("")  # trailing newline

    return "\n".join(lines)


def main():
    report = PipelineReport("from_orphanet")
    print("from_orphanet: fetching Orphanet gene-disease associations...")

    # Check if we have a processed cache already
    cache = load_cache()

    if cache:
        # Use cached processed data
        orphanet_data = {}
        for symbol in sorted(GENES.keys()):
            if symbol in cache:
                orphanet_data[symbol] = cache[symbol]
                disorder_count = len(cache[symbol].get("disorders", []))
                report.cached(symbol, f"{disorder_count} disorders")
                print(f"  {symbol}: cached ({disorder_count} disorders)")
            else:
                report.skipped(symbol, "not in Orphanet")
    else:
        # Download and parse the bulk XML
        try:
            xml_text = download_product6()
        except Exception as e:
            print(f"ERROR: failed to download Orphadata product6: {e}", file=sys.stderr)
            sys.exit(1)

        print("from_orphanet: parsing gene-disease associations...")
        all_gene_disorders = parse_product6(xml_text)
        print(f"  found {len(all_gene_disorders)} genes in Orphadata product6")

        # Filter to our gene list
        orphanet_data = {}
        for symbol in sorted(GENES.keys()):
            if symbol in all_gene_disorders:
                disorders = all_gene_disorders[symbol]
                # Use the first OrphaCode as the representative ID
                orpha_id = disorders[0]["orpha_code"] if disorders else ""
                orphanet_data[symbol] = {
                    "orphanet_id": orpha_id,
                    "disorders": disorders,
                }
                report.ok(symbol, f"{len(disorders)} disorders")
                print(f"  {symbol}: {len(disorders)} disorder(s)")
            else:
                report.skipped(symbol, "not in Orphanet")

        # Save processed cache
        save_cache(orphanet_data)

    if not orphanet_data:
        print("ERROR: no Orphanet data retrieved for any gene", file=sys.stderr)
        sys.exit(1)

    # Write CUE output
    print("from_orphanet: writing model/orphanet.cue...")
    cue_source = generate_cue(orphanet_data)
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, "w") as f:
        f.write(cue_source)

    # Stats
    total_disorders = sum(
        len(d.get("disorders", [])) for d in orphanet_data.values()
    )

    print(f"from_orphanet: wrote {OUTPUT_FILE}")
    print(f"  {len(orphanet_data)} genes, {total_disorders} total disorder associations")
    print(report.summary())


if __name__ == "__main__":
    main()
